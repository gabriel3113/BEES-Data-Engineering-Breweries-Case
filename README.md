# ðŸº Breweries Medallion ETL

Projeto de pipeline ETL usando Apache Airflow, PySpark e o padrÃ£o Medallion Architecture (**Bronze, Silver, Gold**), extraindo e processando dados da [Open Brewery DB API](https://www.openbrewerydb.org/).  
Inclui orquestraÃ§Ã£o de tarefas, particionamento eficiente dos dados e sistema de alertas por e-mail em caso de falha.

---

## ðŸ“‹ SumÃ¡rio

- [VisÃ£o Geral](#visÃ£o-geral)
- [Arquitetura](#arquitetura)
- [Como Rodar Localmente (Docker)](#como-rodar-localmente-docker)
- [ParÃ¢metros & CustomizaÃ§Ã£o](#parÃ¢metros--customizaÃ§Ã£o)
- [Alertas por E-mail (Airflow)](#alertas-por-e-mail-airflow)
- [Estrutura de Pastas](#estrutura-de-pastas)
- [Exemplo de ExecuÃ§Ã£o Manual](#exemplo-de-execuÃ§Ã£o-manual)
- [FAQ e Troubleshooting](#faq-e-troubleshooting)
- [CrÃ©ditos](#crÃ©ditos)

---

## VisÃ£o Geral

Este pipeline realiza:

1. **ExtraÃ§Ã£o** de dados pÃºblicos da Open Brewery DB (API REST)
2. **Armazenamento bruto** (Bronze), **limpeza e normalizaÃ§Ã£o** (Silver) e **agregaÃ§Ã£o** (Gold) dos dados
3. **OrquestraÃ§Ã£o** das etapas via Apache Airflow
4. **Particionamento** eficiente por `state` e `data`
5. **Alertas automÃ¡ticos** por e-mail em caso de falha na DAG

---

## Arquitetura

- **Bronze:** Dados brutos extraÃ­dos da API, salvos como JSON, particionados por estado e data (`state`, `dt`)
- **Silver:** Dados limpos e normalizados com PySpark, persistidos como Parquet, com deduplicaÃ§Ã£o, timestamp de ingestÃ£o e tipagem correta
- **Gold:** Tabelas agregadas por estado e tipo de cervejaria (`brewery_type`), com contagem e datas envolvidas, prontas para anÃ¡lise e visualizaÃ§Ã£o

API -> Airflow (extract.py) -> Bronze (JSON)
|
transform.py
â†“
Silver (Parquet normalizado)
|
load.py
â†“
Gold (Parquet agregado)

yaml
Copiar
Editar

---

## Como Rodar Localmente (Docker)

**PrÃ©-requisitos:**  
- [Docker](https://www.docker.com/) e [Docker Compose](https://docs.docker.com/compose/)

**Passos:**

1. **Clone o repositÃ³rio:**
    ```bash
    git clone https://github.com/seuusuario/breweries-medallion-etl.git
    cd breweries-medallion-etl
    ```

2. **Configure o e-mail de alerta** (opcional, mas recomendado!)  
   Edite o `docker-compose.yml` com seu e-mail e senha de app:

    ```yaml
    environment:
      AIRFLOW__SMTP__SMTP_HOST: "smtp.gmail.com"
      AIRFLOW__SMTP__SMTP_STARTTLS: "True"
      AIRFLOW__SMTP__SMTP_SSL: "False"
      AIRFLOW__SMTP__SMTP_PORT: "587"
      AIRFLOW__SMTP__SMTP_MAIL_FROM: "seu_email@gmail.com"
      AIRFLOW__SMTP__SMTP_USER: "seu_email@gmail.com"
      AIRFLOW__SMTP__SMTP_PASSWORD: "sua_senha_app"
    ```

    > Para Gmail, gere uma [senha de app](https://support.google.com/accounts/answer/185833).

3. **Suba os containers:**
    ```bash
    docker compose up --build
    ```

4. **Acesse o Airflow:**  
    Abra [http://localhost:8080](http://localhost:8080) (usuÃ¡rio/senha padrÃ£o: `airflow` / `airflow`)

5. **Ative a DAG `breweries_medallion_etl` e execute**.

---

## ParÃ¢metros & CustomizaÃ§Ã£o

O pipeline permite processar **intervalos de datas**:

- Para rodar manualmente um intervalo de datas pelo Airflow UI:
    - Clique em "**Trigger DAG w/ config**"  
    - Exemplo de payload:
      ```json
      {
        "start_date": "2025-07-01",
        "end_date": "2025-07-03"
      }
      ```
- Se rodar sem parÃ¢metros, o pipeline processa o dia corrente.

---

## Alertas por E-mail (Airflow)

O projeto jÃ¡ estÃ¡ configurado para enviar alertas de falha por e-mail:

- **Alterar destinatÃ¡rio:** Edite o campo `email` no bloco `default_args` da DAG.
- **CustomizaÃ§Ã£o de mensagem:** Personalize a funÃ§Ã£o de callback, se desejar um corpo de e-mail customizado.

---

## Estrutura de Pastas

.
â”œâ”€â”€ dags/
â”‚ â””â”€â”€ breweries_etl.py # DAG Airflow principal
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ extract.py # ExtraÃ§Ã£o Bronze
â”‚ â”œâ”€â”€ transform.py # NormalizaÃ§Ã£o Silver
â”‚ â””â”€â”€ load.py # AgregaÃ§Ã£o Gold
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ bronze/ # Dados brutos
â”‚ â”œâ”€â”€ silver/ # Dados tratados
â”‚ â””â”€â”€ gold/ # Dados agregados
â”œâ”€â”€ logs/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md

yaml
Copiar
Editar

---

## Exemplo de ExecuÃ§Ã£o Manual

Via terminal, dentro do container:

```bash
# Para extrair um intervalo especÃ­fico de datas
docker exec -it breweries_airflow bash
python src/extract.py --start_date 2025-07-01 --end_date 2025-07-03

# Para transformar apenas um range:
spark-submit src/transform.py --start_date 2025-07-01 --end_date 2025-07-03

# Para gerar o gold apenas para um intervalo:
spark-submit src/load.py 2025-07-01 2025-07-03
FAQ e Troubleshooting
NÃ£o recebo e-mails de erro:

Confira variÃ¡veis SMTP (host, porta, senha, e-mail).

Verifique o log do Airflow e o spam da sua caixa.

Erro de permissÃ£o nos diretÃ³rios /data:

Verifique se a pasta estÃ¡ criada no host e tem permissÃ£o de escrita.

Faltam pacotes no container:

Adicione ao requirements.txt e reinicie com docker compose up --build.

Problemas ao rodar Spark com datasets grandes:

Ajuste o parÃ¢metro N_PARTITIONS no extract.py ou aumente recursos do container.

CrÃ©ditos
Desenvolvido por Seu Nome
Inspirado no modelo Medallion do Databricks e boas prÃ¡ticas de Engenharia de Dados.

Pull requests e sugestÃµes sÃ£o bem-vindas!

yaml
Copiar
Editar

---

Se quiser, posso adaptar ainda mais para o padrÃ£o de README da sua empresa, incluir GIFs/screenshots, instruÃ§Ãµes para rodar no cloud, etc.  
SÃ³ pedir!